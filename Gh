Model Reliance
Overview
Model Reliance measures the degree to which the final output submitted by a Subject Matter Expert (SME) relies on the original response generated by the Large Language Model (LLM). This metric helps assess how much the model's output is trusted and reused versus how much it is modified or overridden by human experts.

It provides insight into:

SME trust in LLM responses

Model overreliance or underuse

Areas where the model may need more tuning or interpretability support

🔍 What It Measures
Model Reliance is computed as the similarity score between the LLM-generated response and the SME-edited submission to Helios. Higher similarity implies more reliance on the model's response.

🎯 Use Cases
Detecting overconfidence in model output (SMEs blindly accepting responses)

Flagging low reliance where the model isn't helpful

Supporting tuning and human-in-the-loop optimization

📏 Thresholds & Color Indicators
Color	Similarity Score Range	Interpretation
🟣 Purple	0.95 – 1.00	Overconfidence: High reuse of model output with minimal/no edits. SME may be over-relying on the model.
🟢 Green	0.75 – 0.94	Healthy Reliance: Model provides strong output, with SMEs making small yet valuable adjustments.
🟠 Amber	0.50 – 0.74	Moderate Reliance: SME is modifying substantial parts. Indicates room for model improvement.
🔴 Red	0.00 – 0.49	Low Reliance: Model output is largely discarded. May signal low model quality or poor prompt alignment.

Similarity is typically measured using cosine similarity between embeddings of the texts (e.g., using sentence-transformers or OpenAI's embedding APIs).

🛠️ Implementation Notes
Ensure consistent pre-processing: lowercase, strip formatting, remove stopwords (optional depending on similarity algorithm).

Exclude boilerplate or template text from both LLM and SME response during similarity computation to avoid inflated scores.

Optionally add SME feedback metadata (e.g., reasons for changes) for more context.

🚦 Actions Based on Threshold
Purple: Add review checkpoints; reinforce SME responsibility; flag for audit.

Green: No action needed; desirable outcome.

Amber: Investigate where edits are made; consider fine-tuning.

Red: Evaluate prompts and model fit for the domain; review training data.

Let me know if you'd like this converted to Markdown, HTML, Confluence format, or integrated into a specific documentation style you’re using.













ChatGPT can make mistakes. Check importa