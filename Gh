In this study, we perform sensitivity analysis on the output generated by a Large Language Model (LLM) that restructures input into a predefined 5C's format (issue, severity, rationale). The goal is to understand how variations in the output affect key evaluation metrics — accuracy, completeness, and hallucination.

Instead of altering the input, we manipulate the output produced by the LLM to simulate potential changes or imperfections in the model’s responses. By doing so, we analyze how these adjustments influence the model's ability to provide consistent, accurate, and complete outputs.

Issue: The "issue" represents the core problem or focus of the input. We modify this component in the LLM's output to see how slight variations impact accuracy (e.g., is the model still correctly identifying the issue?). Sensitivity analysis helps measure whether the LLM's prediction consistency changes when this output component is altered.

Severity: The "severity" component specifies how urgent or serious the issue is. Altering severity in the output (e.g., exaggerating or downplaying the urgency) helps assess how the model handles completeness (whether the severity is appropriately expressed) and hallucination (whether the model generates exaggerated severity levels that were not implied in the original input).

Rationale: The "rationale" explains the reasoning behind the severity of the issue. By modifying this output component, we examine how changes in the reasoning affect hallucination (does the model fabricate or introduce incorrect reasoning?) and accuracy (is the model able to provide a coherent rationale despite the modifications?).

In performing this sensitivity analysis, we adjust the LLM’s output by:

Altering output components: Changing specific parts of the output (issue, severity, rationale) to observe how the metrics vary.

Modifying output content: Editing the content within the output (e.g., rephrasing the severity or rationale) to check if the model still maintains accuracy and completeness.

Removing output components: Omitting certain elements of the output (like severity or rationale) to assess the model's robustness when some details are missing and whether hallucinated content arises.

Through this approach, sensitivity analysis helps us to:

Assess how accurate and complete the LLM remains when output components are altered.

Understand how prone the LLM is to hallucinations when key output elements are modified or omitted.

Evaluate the stability of the model’s performance in generating reliable responses under varying output conditions.

This sensitivity analysis provides valuable insights into the LLM’s performance, ensuring that it remains trustworthy and consistent even when outputs are varied.
