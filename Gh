We cannot depend on a separate Python script to calculate scores on the LLM’s output because the model’s responses are non-deterministic. This means the same request may produce slightly different answers each time, which can cause the Python script to misread or mis-score the results, leading to unreliable metrics. Instead, it is more effective to let the LLM itself provide the final scores in a controlled format. This ensures scoring is consistent, aligned with the model’s own understanding, and reduces the risk of incorrect evaluations.”