# Ensure you have the necessary packages installed:
# pip install langchain langchain-google-vertexai google-cloud-aiplatform

import os
from langchain_google_vertexai import ChatVertexAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage

def main():
    """
    Demonstrates how to use ChatVertexAI from langchain_google_vertexai
    to interact with a chat model on Google Cloud Vertex AI (e.g., Gemini).
    """

    # --- Configuration ---
    # 1. Authentication:
    #    Make sure your environment is authenticated to Google Cloud.
    #    This typically involves:
    #    a) Running `gcloud auth application-default login`
    #    b) Or, setting the GOOGLE_APPLICATION_CREDENTIALS environment variable
    #       to the path of your service account key JSON file.
    #    Langchain will use these credentials automatically.

    # 2. Project and Location:
    #    You might need to specify your Google Cloud project ID and location (region).
    #    These can often be inferred from the environment if `gcloud` is configured,
    #    or you can set them explicitly.
    # os.environ["GOOGLE_CLOUD_PROJECT"] = "your-gcp-project-id"
    # os.environ["GOOGLE_CLOUD_LOCATION"] = "your-gcp-region" # e.g., "us-central1"

    # 3. Model Name:
    #    Specify the Vertex AI model name. For Gemini, it might be something like:
    #    - "gemini-1.5-flash-001"
    #    - "gemini-1.5-pro-001"
    #    - "gemini-1.0-pro-002" (ensure the model supports chat/multi-turn)
    #    Check the Vertex AI documentation for the latest available model identifiers.
    model_name = "gemini-1.5-flash-001" # Replace with your desired Gemini model on Vertex AI

    # --- Initialize ChatVertexAI ---
    try:
        llm = ChatVertexAI(
            model_name=model_name,
            project=os.getenv("GOOGLE_CLOUD_PROJECT"), # Optional: if not set in env
            location=os.getenv("GOOGLE_CLOUD_LOCATION"), # Optional: if not set in env
            # You can add other parameters like temperature, top_p, max_output_tokens, etc.
            temperature=0.7,
            max_output_tokens=1024,
            # streaming=True, # Set to True if you want to stream responses
        )
        print(f"Successfully initialized ChatVertexAI with model: {model_name}")
    except Exception as e:
        print(f"Error initializing ChatVertexAI: {e}")
        print("Please ensure your Google Cloud credentials and project/location are set up correctly,")
        print("and the specified model name is valid for your project and region.")
        return

    # --- Basic Invocation (Single Turn) ---
    print("\n--- Basic Invocation (Single Turn) ---")
    try:
        prompt_text = "What is the largest planet in our solar system and why is it considered so?"
        human_message = HumanMessage(content=prompt_text)
        print(f"Sending prompt: \"{prompt_text}\"")

        # .invoke() sends the request and gets the full response
        ai_response = llm.invoke([human_message])

        print("\nLLM Response:")
        print(ai_response.content)
    except Exception as e:
        print(f"Error during basic invocation: {e}")

    # --- Using with ChatPromptTemplate and LCEL (Langchain Expression Language) ---
    print("\n--- Using with ChatPromptTemplate and LCEL ---")
    try:
        # Define a chat prompt template
        # This allows for structured multi-turn conversations
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessage(content="You are a helpful assistant that translates English to French."),
            HumanMessage(content="{english_text}")
        ])

        # Create a simple chain using LCEL
        chain = prompt_template | llm

        english_input = "Hello, how are you today?"
        print(f"Sending English text for translation: \"{english_input}\"")

        # Invoke the chain
        translation_response = chain.invoke({"english_text": english_input})

        print("\nLLM Translation Response (French):")
        print(translation_response.content)

    except Exception as e:
        print(f"Error using ChatPromptTemplate and LCEL: {e}")

    # --- Streaming Example (Optional) ---
    # To use streaming, initialize llm with streaming=True
    # llm_streaming = ChatVertexAI(model_name=model_name, streaming=True)
    # print("\n--- Streaming Example ---")
    # try:
    #     chunks = []
    #     print(f"Streaming response for prompt: \"{prompt_text}\"")
    #     for chunk in llm_streaming.stream([HumanMessage(content=prompt_text)]):
    #         chunks.append(chunk.content)
    #         print(chunk.content, end="", flush=True)
    #     print("\n--- End of Stream ---")
    #     full_streamed_response = "".join(chunks)
    #     # print(f"Full streamed response: {full_streamed_response}")
    # except Exception as e:
    #     print(f"Error during streaming: {e}")


if __name__ == "__main__":
    # Note: To run this, you need to have your Google Cloud authentication
    # set up (e.g., via `gcloud auth application-default login` or service account key).
    # You might also need to set GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION
    # environment variables if not configured in your gcloud sdk.
    print("Attempting to run ChatVertexAI example...")
    print("If you encounter authentication or permission errors, please check your GCP setup.")
    main()

