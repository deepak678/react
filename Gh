from langchain.llms import BaseLLM

class GeminiLLM(BaseLLM):
    def __init__(self, api_key: str, endpoint: str):
        self.api_key = api_key
        self.endpoint = endpoint

    def _call(self, prompt: str, stop: list = None) -> str:
        # Logic to send the prompt to Gemini API and fetch the response
        response = requests.post(
            self.endpoint,
            json={"prompt": prompt, "api_key": self.api_key}
        )
        return response.json()["output"]

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Define a prompt template
prompt_template = PromptTemplate(
    input_variables=["question"],
    template="Answer the question based on the provided context: {question}",
)

# Load Gemini LLM
gemini_llm = GeminiLLM(api_key="your_api_key", endpoint="https://gemini-model.api/endpoint")

# Create an LLM chain
llm_chain = LLMChain(llm=gemini_llm, prompt=prompt_template)

# Test the chain
question = "What is the capital of France?"
response = llm_chain.run(question=question)
print(response)




