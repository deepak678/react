Red: < 5th Percentile (Manual)
Yellow: 5th Percentile (Manual) <= Score < 25th Percentile (Manual) (or maybe median/50th)
Green: >= 25th Percentile (Manual) (or maybe median/50th)

Collect Manual Completeness Scores: You already have these from calculating the 5th percentile: C_manual = {c_1, c_2, ..., c_m}. Ensure they are based on a clear, consistent rubric for completeness.
Choose a High Percentile: Decide what level of performance constitutes "unusually high" compared to your human evaluators. Common choices are:
95th Percentile: The score exceeded by only the top 5% of your manual evaluations. This flags results better than most human-scored examples.
99th Percentile: The score exceeded by only the top 1% of your manual evaluations. This flags truly exceptional (or potentially anomalous) results compared to the human baseline.
Let's use the 95th percentile for this example.
Calculate the Chosen High Percentile:
Using your sorted manual completeness scores (C_manual), calculate the 95th percentile value.
Use standard functions: PERCENTILE.INC(data_range, 0.95) in Excel/Sheets or numpy.percentile(scores, 95) in Python.
Set the "Overconfidence" / "Unusually High" Threshold:
The calculated 95th percentile (or your chosen percentile) becomes the lower bound for this zone.
Threshold_Overconfidence = 95th_Percentile(C_manual)
Define the Zone:
Outputs with a completeness score above this threshold fall into your "Overconfidence" or "Unusually High" category.
Overconfidence / Unusually High: LLM_Completeness_Score > Threshold_Overconfidence
Red (Not Okay / Incomplete): Score < 5th Percentile (Manual)
Yellow (Concern / Minimally Complete): 5th Percentile (Manual) <= Score < 25th Percentile (Manual) (or maybe 50th/Median)
Green (Okay / Good Completeness): 25th Percentile (Manual) <= Score <= 95th Percentile (Manual) (adjust lower bound as needed)
Blue (Overconfident / Unusually High): Score > 95th Percentile (Manual)
