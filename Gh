Goals

Display total erratic feedback count prominently on Admin Portal Home.

Provide direct navigation to Model Evaluation Performance page filtered by erratic feedback records.

Ensure consistency between the count displayed on the Home page and the list of records shown after drill-down.

Enable stakeholders to quickly track and investigate erratic feedback cases.

Scope

In-scope

Backend: Aggregate erratic feedback count.

Frontend: Add KPI card on Admin Portal Home, clickable with redirect.

Model Evaluation Performance page: Add filtering support for erratic_feedback.

Ensure filters work with other metrics (request type, reporting month, request location, date range).

Out-of-scope

Defining “erratic feedback” logic (assumes flag already exists in DB).

Enhancing feedback classification logic (only display & drill-down functionality).

Acceptance Criteria

Home Page Card

A KPI card titled “Erratic Feedback” is shown on Admin Portal Home.

It displays the total count of erratic feedback records within the selected time range (default: last 30 days).

Clickable Navigation

Clicking the card redirects user to Model Evaluation Performance page.

The page is pre-filtered to show only erratic_feedback = true records.

Consistency

Count on the Home Page matches the number of rows returned on the Model Evaluation Performance page (for the same filters).

Filters

Default filters (reporting month, request type, request location, date range) apply to erratic feedback counts.

Users can modify filters on the Model Evaluation page to widen/narrow results.

UI/UX

KPI card styled consistently with other dashboard metrics (rounded corners, shadow, trend indicator).

Tooltip: “Total count of erratic feedback cases flagged by evaluation logic.”

Testing

Unit tests for count calculation API.

Integration tests validating card ↔ drill-down navigation.

Manual test case: card count equals table rows.

User Stories & Tasks
A. Backend

Story A1 — API for Erratic Feedback Count

Task A1.1: Extend metrics API: GET /metrics/erratic_feedback?filters... → {count: 123}.

Task A1.2: Ensure query respects filters (reporting month, request type, request location, date range).

Story A2 — Model Evaluation Data API

Task A2.1: Add query parameter erratic_feedback=true to existing /model_evaluation endpoint.

Task A2.2: Ensure pagination + sorting work with filter.

B. Frontend

Story B1 — Home Page Card

Task B1.1: Add KPI card “Erratic Feedback” alongside other metrics.

Task B1.2: Display count with trend (optional: % change vs previous period).

Task B1.3: Add tooltip with definition.

Story B2 — Navigation to Drill-down

Task B2.1: Make card clickable → redirect to Model Evaluation Performance page.

Task B2.2: Pre-apply filter erratic_feedback = true.

Story B3 — Model Evaluation Page Update

Task B3.1: Add filter toggle for erratic_feedback.

Task B3.2: Ensure UI updates (badge/tag on filters to show “Erratic Feedback applied”).

Task B3.3: Display rows in table with same styling as other metrics.

C. QA / Observability

Test: Card count = number of rows on drill-down (with same filters).

Test: Removing filter shows all rows.

Telemetry: log click-throughs from card → drill-down for usage analytics.

Data Model (assumption)

If not already available, flag in DB:

ALTER TABLE model_evaluation
ADD COLUMN erratic_feedback BOOLEAN DEFAULT false;

API Contract (examples)

Erratic Feedback Count
GET /api/v1/metrics/erratic_feedback?start=2025-09-01&end=2025-09-14&filters=...
→ {count: 48}

Model Evaluation Drill-down
GET /api/v1/model_evaluation?erratic_feedback=true&filters=...
→ [ {id: "...", request_id: "...", erratic_feedback: true, ...}, ...]

Deliverables

Home page KPI card (Erratic Feedback).

Backend API for count & drill-down.

Model Evaluation Performance page with erratic feedback filter.

Tests & validation ensuring data consistency.

Documentation update (Admin Portal user guide).

Risks & Mitigations

Mismatch between Home count and drill-down list → enforce same filters & date range for both.

Performance overhead for count query → add DB index on erratic_feedback, computed_at.

User confusion on definition → provide tooltip + docs.

Rollout Plan

Backend API ready.

Frontend KPI card added in staging.

End-to-end test: click card → drill-down matches count.

UAT with business stakeholders.

Production rollout + monitor telemetry.

Do you want me to also merge this new epic with the earlier “Model Reliance” epic into a single Release Epic (Release 3 expansion of Admin Portal), or keep them separate epics under one parent initiative