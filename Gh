from openai import ChatCompletion

# Function to evaluate with G-Eval
def evaluate_g_eval(input_text, output_text, evaluation_type):
    if evaluation_type == "context":
        prompt = f"""
        The input text is:
        "{input_text}"

        The output text is:
        "{output_text}"

        Evaluate how well the output restructures and organizes the content from the input into the given framework. 
        Rate the context relevance on a scale from 1 (completely irrelevant) to 5 (highly relevant) and explain your reasoning.
        """
    elif evaluation_type == "semantic":
        prompt = f"""
        The input text is:
        "{input_text}"

        The output text is:
        "{output_text}"

        Evaluate if the output preserves all critical details and meaning from the input without adding new, unrelated information. 
        Rate the semantic relevance on a scale from 1 (low) to 5 (high) and explain your reasoning.
        """
    
    # Use the LLM for evaluation
    response = ChatCompletion.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are an expert evaluator for generative AI outputs."},
            {"role": "user", "content": prompt}
        ]
    )
    
    return response["choices"][0]["message"]["content"]

# Example input and output
input_text = "Regular physical activity improves health and reduces disease risks."
output_text = "Exercise helps in improving overall health and reducing the risks of chronic diseases."

# Evaluate context and semantic relevance
context_score = evaluate_g_eval(input_text, output_text, "context")
semantic_score = evaluate_g_eval(input_text, output_text, "semantic")

print("Context Relevance Evaluation:", context_score)
print("Semantic Relevance Evaluation:", semantic_score)
