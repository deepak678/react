import requests
from langchain.llms import BaseLLM
from typing import List, Dict, Any

class GeminiLLM(BaseLLM):
    def __init__(self, api_key: str, endpoint: str, proxy_url: str, timeout: int = 10):
        """
        Initialize the GeminiLLM class with API key, endpoint, and proxy settings.
        
        :param api_key: The API key for accessing the Gemini model.
        :param endpoint: The endpoint of the Gemini model.
        :param proxy_url: The Azure proxy URL.
        :param timeout: Request timeout in seconds (default is 10).
        """
        self.api_key = api_key
        self.endpoint = endpoint
        self.proxy_url = proxy_url
        self.timeout = timeout

    @property
    def _llm_type(self) -> str:
        """
        Return the type of LLM.
        """
        return "gemini"

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        """
        Send the prompt to the Gemini model via the Azure proxy and fetch the response.

        :param prompt: The input prompt for the model.
        :param stop: Optional stop tokens.
        :return: The model's response as a string.
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "prompt": prompt,
            "stop": stop or [],
        }

        proxies = {
            "http": self.proxy_url,
            "https": self.proxy_url,
        }

        try:
            response = requests.post(
                self.endpoint,
                json=payload,
                headers=headers,
                proxies=proxies,
                timeout=self.timeout,
            )
            response.raise_for_status()
            return response.json().get("output", "No output received from the model.")
        except requests.exceptions.RequestException as e:
            raise ValueError(f"Error communicating with the Gemini model: {e}")

    def _generate(self, prompts: List[str], stop: List[str] = None) -> List[Dict[str, Any]]:
        """
        Generate a response for a list of prompts.

        :param prompts: A list of input prompts.
        :param stop: Optional stop tokens.
        :return: A list of dictionaries containing the response for each prompt.
        """
        results = []
        for prompt in prompts:
            output = self._call(prompt, stop=stop)
            results.append({"text": output})
        return results
