from nltk.tokenize import sent_tokenize

# Tokenize output into sentences
output_sentences = sent_tokenize(output_text)

# Compute similarity for each sentence
hallucination_flags = []
for sentence in output_sentences:
    sentence_embedding = get_embedding(sentence)
    similarity = np.dot(input_embedding, sentence_embedding) / (
        np.linalg.norm(input_embedding) * np.linalg.norm(sentence_embedding)
    )
    hallucination_flags.append(similarity < 0.6)  # Threshold: 0.6

# Count hallucinated sentences
hallucination_count = sum(hallucination_flags)
print(f"Hallucinated sentences: {hallucination_count} out of {len(output_sentences)}")
